### ***Final Project - Modeling Analysis Feedback***

***Nico Van de Bovenkamp***
***

**Overall** Awesome, awesome project! You are making great use of many of the techniques we used in class, in particular those odds ratios! Actually showing odds ratios and using them (outside of some weibul tests/other statistical tests to evaluate parameter estimation) is not common, but this looks fantastic. Very interesting to see how that measures against `feature_importance_` in a Random Forest, which is definitely a huge point to bring up in your final report.

*Quick note: I did not know about `classification_report()` but I will definitely be making use of that! Also, as you can see, base rate is a huge challenge in this problem*

**Some Thoughts**
* **Base Rates:** As we previously discussed, when there is a clear dominance of classes, you have to deal with this in your model (and when considering what model evaluation metrics to use: AUC, Precision, Recall, etc.!) Thankfully, sklearn does a good bit of work for you and has a way you can pass the class label and use a dictionary to re-weight the dominate class! Check out these two guides: [SVM Guide](http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html), [ a nice description of what weighting really means/why you would do it](http://stackoverflow.com/questions/30972029/how-does-the-class-weight-parameter-in-scikit-learn-work). Finally, another great method is to downsample. Of course, if you do downsample the dominate class, you do lose out on many valuable instances. Still, the technique has been proven to work if done well! [take a look at this guide](https://svds.com/learning-imbalanced-classes/)
* **AUC Scores and ROC Curves:**  Tagging on to the discussion of base rates, the great thing about AUC Scores is that they are **base rate invariant!** In other words, if you have a largely dominant class, AUC scores will show you the probability of actually scoring a positive class over a negative class. You see this when you run that final grid search/cross validation. The model is above .54, which is better than random!
* **Cross Validation:** I noticed that you are often using just 3 fold validation. I recommend that you increase the folds. Given the number of instances you have, you will greatly reduce your model variance by increasing the number of folds. In the case of tree based models, as well as other non-linear models, this is key.
* **Data Transformations and Scaling** Many of your predictors are on different scales, and they may be slightly skewed. To deal with this, I recommend you implement some data transformations and/or scale your data! [Check out this quick guide!](https://www.isixsigma.com/tools-templates/normality/tips-recognizing-and-transforming-non-normal-data/). [This one is also quite good](https://www.isixsigma.com/tools-templates/normality/making-data-normal-using-box-cox-power-transformation/).
* **Random Forest:** Nice work with the modeling! I highly recommend you add in some hyper-parameters and perform a Grid Search (with the cross validation) and then evaluate that fit: **min-leaf-size or min-split-size will help**, and add more estimators (you can probably add like another hundred)! The typical approach for random forests (and this is even more true of Gradient Boosted Trees) is to have trees that are a bit shorter and then add many estimators. This approach takes advantage of the feature bagging that random forests do (taking subsets of features to learn on) and then, when add more estimators, the variance is balanced out with each model!
* **Model Stacking** Another technique/model architecture that is quite good is to make use of [Stacked Models](https://www.kaggle.com/arthurtok/titanic/introduction-to-ensembling-stacking-in-python). The architecture benefits from the different ways models learn, and aggregates them for better predictions. I would recommend fitting an SVM and a Logistic Regression (cross validated and hyper-parameter tuned), then output the predictions for each model, THEN fit a random forest on your training set with the predictions of each model in your feature set.

***If all else fails, find more data! I am not quite sure how you would do it in this case, but if there was some key feature related to Zip Codes or Census tract or location, you would be able to contextualize the problem (and features) much better. Of course, though, this can go down a rabbit hole of discrimination depending on your use case. Boy, these problems are hard!***
